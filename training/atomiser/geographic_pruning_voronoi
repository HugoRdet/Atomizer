"""
Geographic Attention Module (Randomized Voronoi Sampling)

This version ensures that tokens are partitioned among latents (Voronoi cells).
Each latent then samples k tokens randomly from its own geographic territory.
"""

import torch
import torch.nn as nn
from typing import Tuple, Optional

class GeographicPruning(nn.Module):
    def __init__(
        self,
        geometry,
        num_spatial_latents: int,
        geo_k: int = 1500,
        assign_k: int = 4,
        default_sigma: float = 0.5,
    ):
        super().__init__()
        self.geometry = geometry
        self.num_spatial_latents = num_spatial_latents
        self.geo_k = geo_k
        self.assign_k = assign_k
        self.default_sigma = default_sigma

    def forward(
        self,
        tokens: torch.Tensor,
        mask: torch.Tensor,
        latent_coords: torch.Tensor,
        sigma: Optional[float] = None,
        temperature: float = 0.1,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        B, N, D = tokens.shape
        L = self.num_spatial_latents
        k = self.geo_k
        device = tokens.device
        calc_dtype = torch.bfloat16

        if torch.isnan(latent_coords).any():
            latent_coords = torch.nan_to_num(latent_coords, nan=0.0)

        with torch.no_grad():
            # 1. Global Pixel Mapping
            pixel_coords = self.geometry.get_token_centers(tokens).to(dtype=calc_dtype)
            curr_latent_coords = latent_coords.to(dtype=calc_dtype)

            # 2. Manual L2 Distance Computation [B, L, N]
            diff = curr_latent_coords.unsqueeze(2) - pixel_coords.unsqueeze(1)
            dist_sq = torch.sum(diff.pow(2), dim=-1)
            
            # 3. Voronoi Partitioning
            # Every token is assigned to exactly one nearest latent.
            # nearest_latent_idx shape: [B, N]
            nearest_latent_idx = dist_sq.argmin(dim=1) 

            # 4. Stochastic Weighting for Sampling
            # Create a mask where tokens not belonging to a latent have near-zero weight.
            # cell_mask shape: [B, L, N]
            cell_mask = torch.zeros_like(dist_sq, dtype=calc_dtype)
            cell_mask.scatter_(1, nearest_latent_idx.unsqueeze(1), 1.0)
            
            # Add a small epsilon to allow latents to "borrow" tokens if their cell 
            # contains fewer than k tokens (important for low-density areas).
            sampling_weights = cell_mask + 1e-6 

            # 5. Multinomial Sampling
            # Samples k tokens per latent from the prioritized weights.
            # Reshape for bulk sampling: [B*L, N] -> [B*L, k]
            selected_indices = torch.multinomial(
                sampling_weights.view(B * L, N), 
                num_samples=k, 
                replacement=False
            ).view(B, L, k)
            
            # 6. Distance-Aware Attention Bias
            # We use actual distances to ensure local spatial geometry is respected.
            actual_dist_sq = torch.gather(dist_sq, dim=-1, index=selected_indices)
            
            # Using a tighter sigma for partitioned tokens often helps reconstruction
            effective_sigma = sigma if sigma is not None else self.default_sigma
            selected_bias = -actual_dist_sq / (2 * (effective_sigma ** 2))

        # 7. Gather tokens and masks
        tokens_per_latent = self._gather_tokens(tokens, selected_indices)
        masks_per_latent = self._gather_masks(mask, selected_indices)

        return tokens_per_latent, masks_per_latent, selected_bias.to(tokens.dtype)

    def _gather_tokens(self, tokens: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:
        B, N, D = tokens.shape
        L, k = indices.shape[1], indices.shape[2]
        flat_indices = indices.reshape(B, L * k)
        flat_indices_exp = flat_indices.unsqueeze(-1).expand(-1, -1, D)
        gathered = torch.gather(tokens, dim=1, index=flat_indices_exp)
        return gathered.reshape(B, L, k, D)

    def _gather_masks(self, mask: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:
        B, N = mask.shape
        L, k = indices.shape[1], indices.shape[2]
        flat_indices = indices.reshape(B, L * k)
        gathered = torch.gather(mask, dim=1, index=flat_indices)
        return gathered.reshape(B, L, k).bool()
    

def create_geographic_pruning(config: dict, geometry) -> GeographicPruning:
    atomiser_cfg = config["Atomiser"]
    spatial_latents_per_row = atomiser_cfg["spatial_latents"]
    
    return GeographicPruning(
        geometry=geometry,
        num_spatial_latents=spatial_latents_per_row ** 2,
        geo_k=atomiser_cfg.get("geo_k", 1500),
        assign_k=atomiser_cfg.get("assign_k", 4), # Added this line
        default_sigma=atomiser_cfg.get("geo_sigma", 0.5),
    )